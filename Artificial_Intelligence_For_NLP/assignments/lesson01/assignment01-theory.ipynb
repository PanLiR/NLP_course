{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分 \n",
    "** 0.Can you come up out 3 sceneraies which use AI methods? **  \n",
    "**Ans: ** Siri智能回复、扫地机器人、天猫精灵\n",
    "\n",
    "** 1.How do we use Github; Why do we use Jupyter and Pycharm？ **  \n",
    "**Ans:** 老师使用Github来共享课程资料（包括课件、视频、作业和代码等），学生用来跟进课程进度、提交作业和课程反馈，助教用来评阅学生作业，总之，课程中Github为师生提供共享学习资料的平台。 \n",
    "  \n",
    "Jupyter本质上是一个开源的Web应用程序，使用 Jupyter方便代码的单行测试，并能实时地给出测试结果（包括图像等），同时可以通过电子邮件和Github等将Jupyter文件共享给其他人，便于实时教学和共享代码。  \n",
    "\n",
    "我们之所以使用Pycharm是基于其一下几个优点：  \n",
    "1.可以同时运行多个文件，并为每个文件提供其输出窗口  \n",
    "2.可以终止进程  \n",
    "3.代码的提示清晰：一，没用的变量颜色会变灰；二，用错的变量下面会有红色波浪线；三，书写提示  \n",
    "4.索引功能强\n",
    "\n",
    "** 2.What's the Probability Model? **  \n",
    "**Ans:** Probability Model，即统计模型，这里是指概率模型（probabilistic model）。概率模型，即非确定性模型，其输入和输出均可视为随机变量。模型必须能准确“捕获”样本数据的内在统计规律，即$P(x)$必须是真实概率分布的近似，我们的任务就是估算数据集的概率密度或概率分布。  \n",
    "\n",
    "如果我们选取假想的$P(x)$是参数型的概率分布，那么，我们的目标是通过观察到的样本数据集X获得样本分布的参数集$\\theta$。  \n",
    "\n",
    "我们从贝叶斯公式出发：  \n",
    "$$P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}$$\n",
    "其中，$P(\\theta)$是分布参数$\\theta$的先验概率，即我们一开始所做的假设。数据集X会在$P(\\theta)$之上不断调整$\\theta$的分布，直至它足够接近真正的概率分布，即其后验概率分布。  \n",
    "\n",
    "$P(X|\\theta)$是似然概率，即在当前参数$\\theta$下，出现我们观察到的数据X的概率。如果数据集里面有多个样本，我们假设他们都是iid的，这样当我们计算似然概率时，可以但单独计算每个样本出现的似然概率，再把他们相乘。  \n",
    "\n",
    "**3. Can you came up with some sceneraies at which we could use Probability Model?**  \n",
    "**Ans:** 股市收益率预测、人脸识别等。  \n",
    "\n",
    "**4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?**  \n",
    "**Ans:** 概率论是表示不确定性的框架，它不仅提供量化不确定性的方法，也提供了用于导出新的不确定性声明的公理。概率论能使我们做出不确定的声明以及在不确定性存在的情况下进行推理，而信息论能让我们量化概率分布中不确定性的总量。  \n",
    "\n",
    "因为机器学习通常必须处理不确定量和随机量，不确定性有三种可能的来源：\n",
    "1.被建模系统内在的随机性。如一个假想的纸牌游戏，在这个游戏中我们假设纸牌真正混洗成了随机顺序。  \n",
    "2。不完全预测。即便是确定的系统，当我们不能观测到所有驱动系统行为的变量时，该系统也会呈现随机性。  \n",
    "3.不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会导致模型的预测出现不确定性。\n",
    "\n",
    "基于解析和模式匹配的编程难点在于：\n",
    "1.在模式复杂的情况下，很难识别到模式的所有特征，只能考虑其关键特征；\n",
    "2.在确定关键特征后，如何建立模型来识别和模拟，同时该模型在多大程度上能准确识别。\n",
    "\n",
    "**5.What's the Language Model**   \n",
    "**Ans:** 一个语言模型通常构建为字符串s的概率分布$P(s)$，这里的$P(s)$实际上反映的是s作为一个句子出现的概率。  \n",
    "\n",
    "这里的概率指的是组成字符串的这个组合，在训练语料库中出现的似然概率，与句子是否合乎语法无关。假设训练语料库来自于人类的语言，那么可以认为这个概率指的是一句话是否为人话的概率。 \n",
    "\n",
    "**6.Can you came up with some sceneraies at which we could use Language Model?**\n",
    "**Ans:** 垃圾邮件甄别，智能语音识别、机器翻译等等。  \n",
    "\n",
    "**7.What's the 1-gram language mode**\n",
    "**Ans:** 1-gram语言模型，即一元语言模型，是指在语言模型中，假定一个词出现的概率与它前后出现的词是相互独立的。假设一个句子s由t个词组成，$s = w_1w_2...w_t$,则有  \n",
    "$$P(s) = P(w_1w_2...w_t) = P(w_1)P(w_2)...P(w_t)$$  \n",
    "\n",
    "**8.What's the disadvantages and advantages of 1-gram language model**\n",
    "**Ans:** 一元语言模型的优点是：概率估计的分子和分母都不会出现 0 的情况，因此这个概率估计就非常好定义，并且该值是永远大于 0（我们假设每个词在训练语料库中出现的次数至少一次，这个假设是可信的）。  \n",
    "\n",
    "一元语言模型的缺点是：一元语言模型完全的忽视了上下文信息，也就是说丢弃了很多有价值的信息。  \n",
    "\n",
    "**9.What't the 2-gram models？**  \n",
    "**Ans:** 2-gram语言模型，即二元语言模型，指在一个语言模型中，假设一个词的出现仅与它前面的一个词有关时，此时自由参数数量级是V^2。假设一个句子s由t个词组成，$s = w_1w_2...w_t$,则有  \n",
    "$$P(s) = P(w_1w_2...w_t) = P(w_1)P(w_2|w_1)P(w_3|w_2)...$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
